{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR CONV\n",
    "\n",
    "\n",
    "[Explain about data]\n",
    "[Explain about convolutional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mlp.data_providers import CIFAR10DataProvider, CIFAR100DataProvider\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float32\n",
    "  var =  tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "            shape), \n",
    "        'weights')\n",
    "\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 200\n",
    "num_hidden2 = 100\n",
    "num_hidden3 = 50\n",
    "BATCH_SIZE = 50\n",
    "NUM_CLASSES = 10\n",
    "input_dim = 32\n",
    "output_dim = 32\n",
    "EPOCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = CIFAR10DataProvider('train', batch_size=BATCH_SIZE)\n",
    "train_data.inputs = train_data.inputs.reshape((-1, 32, 32, 3))\n",
    "valid_data = CIFAR10DataProvider('valid', batch_size=BATCH_SIZE)\n",
    "valid_data.inputs = valid_data.inputs.reshape((-1, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# place holder for input and target\n",
    "inputs = tf.placeholder(tf.float32, [None, train_data.inputs.shape[1], train_data.inputs.shape[2], train_data.inputs.shape[3]], 'inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, train_data.num_classes], 'targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we try to build a simple convolutional Neural network as a baseline of our next model.\n",
    "this network consist of one convolutional layer - one dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# building graph\n",
    "\n",
    "conv1_out_size = 14 #number of output channel of first convolutional \n",
    "with tf.name_scope('conv-1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 3, conv1_out_size],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "\n",
    "    conv = tf.nn.conv2d(inputs, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    #biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    biases = tf.Variable(tf.zeros([conv1_out_size]), 'biases') \n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    # conv1 = tf.nn.relu(pre_activation)\n",
    "    local1 = tf.nn.relu(pre_activation)\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(local1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "# Move everything into depth so we can perform a single matrix multiply.\n",
    "with tf.name_scope('Dense-Relu_Layer') as scope:\n",
    "    # flattening the input\n",
    "    tot_shape=pool1.get_shape()[1].value*pool1.get_shape()[2].value*pool1.get_shape()[3].value\n",
    "    reshape = tf.reshape(pool1, [BATCH_SIZE,tot_shape])\n",
    "    weights = _variable_with_weight_decay('weights3', shape=[tot_shape, 384],stddev=1.0, wd=0.0)\n",
    "    biases = tf.Variable(tf.zeros([384]), 'biases') \n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases)\n",
    "\n",
    "\n",
    "with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights5', [384, NUM_CLASSES],\n",
    "                                          stddev=1.0, wd=0.0)\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]), 'biases') \n",
    "    softmax_linear = tf.add(tf.matmul(local3, weights), biases)\n",
    "    soft_max_out = tf.nn.softmax(softmax_linear)\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    out_login = tf.nn.softmax_cross_entropy_with_logits(logits = softmax_linear, labels=targets)\n",
    "    error = tf.reduce_mean(out_login)\n",
    "\n",
    "# use softmax for accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(\n",
    "            tf.equal(tf.argmax(soft_max_out, 1), tf.argmax(targets, 1)), \n",
    "            tf.float32))f\n",
    "\n",
    "# use adam optimizer \n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer().minimize(error)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary to save all the data\n",
    "all_train_score_d = {}\n",
    "all_valid_score_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 01: err(train)=291.29 acc(train)=0.18\n",
      "                 err(valid)=259.70 acc(valid)=0.27\n",
      "End of epoch 02: err(train)=204.64 acc(train)=0.29\n",
      "                 err(valid)=171.76 acc(valid)=0.29\n",
      "End of epoch 03: err(train)=137.45 acc(train)=0.31\n",
      "                 err(valid)=120.11 acc(valid)=0.30\n",
      "End of epoch 04: err(train)=98.46 acc(train)=0.31\n",
      "                 err(valid)=89.21 acc(valid)=0.30\n",
      "End of epoch 05: err(train)=73.11 acc(train)=0.32\n",
      "                 err(valid)=66.85 acc(valid)=0.28\n",
      "End of epoch 06: err(train)=49.16 acc(train)=0.29\n",
      "                 err(valid)=37.47 acc(valid)=0.25\n",
      "End of epoch 07: err(train)=25.50 acc(train)=0.24\n",
      "                 err(valid)=19.48 acc(valid)=0.21\n",
      "End of epoch 08: err(train)=12.94 acc(train)=0.19\n",
      "                 err(valid)=10.39 acc(valid)=0.17\n",
      "End of epoch 09: err(train)=8.22 acc(train)=0.18\n",
      "                 err(valid)=7.33 acc(valid)=0.18\n",
      "End of epoch 10: err(train)=5.92 acc(train)=0.19\n",
      "                 err(valid)=5.54 acc(valid)=0.19\n",
      "End of epoch 11: err(train)=4.58 acc(train)=0.20\n",
      "                 err(valid)=4.38 acc(valid)=0.20\n",
      "End of epoch 12: err(train)=3.68 acc(train)=0.21\n",
      "                 err(valid)=3.58 acc(valid)=0.21\n",
      "End of epoch 13: err(train)=3.06 acc(train)=0.23\n",
      "                 err(valid)=3.06 acc(valid)=0.22\n",
      "End of epoch 14: err(train)=2.64 acc(train)=0.24\n",
      "                 err(valid)=2.68 acc(valid)=0.24\n",
      "End of epoch 15: err(train)=2.35 acc(train)=0.25\n",
      "                 err(valid)=2.46 acc(valid)=0.24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-db685f808d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             _, batch_error, batch_acc = sess.run(\n\u001b[1;32m     17\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 feed_dict={inputs: input_batch, targets: target_batch})\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m# calculating error and accuracy for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrunning_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/inf.ed.ac.uk/user/s15/s1575408/miniconda2/envs/mlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/inf.ed.ac.uk/user/s15/s1575408/miniconda2/envs/mlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/inf.ed.ac.uk/user/s15/s1575408/miniconda2/envs/mlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/afs/inf.ed.ac.uk/user/s15/s1575408/miniconda2/envs/mlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/inf.ed.ac.uk/user/s15/s1575408/miniconda2/envs/mlp/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# begin training\n",
    "experiment_name = \"(1) simple conv-net\"\n",
    "acc_train_list = []\n",
    "err_train_list = []\n",
    "acc_valids = []\n",
    "err_valids = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for e in range(EPOCH_SIZE):\n",
    "        running_error = 0.\n",
    "        running_accuracy = 0.\n",
    "        for input_batch, target_batch in train_data:            \n",
    "            _, batch_error, batch_acc = sess.run(\n",
    "                [train_step, error, accuracy], \n",
    "                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "            # calculating error and accuracy for batch\n",
    "            running_error += batch_error\n",
    "            running_accuracy += batch_acc\n",
    "        # averaging the error and accuracy\n",
    "        running_error /= train_data.num_batches\n",
    "        running_accuracy /= train_data.num_batches\n",
    "        print('End of epoch {0:02d}: err(train)={1:.2f} acc(train)={2:.2f}'\n",
    "              .format(e + 1, running_error, running_accuracy))\n",
    "        acc_train_list.append(running_accuracy)\n",
    "        err_train_list.append(running_error)\n",
    "\n",
    "        # validation\n",
    "        if  (e + 1) % 1 == 0:\n",
    "            valid_error = 0.\n",
    "            valid_accuracy = 0.\n",
    "            for input_batch, target_batch in valid_data:\n",
    "                batch_error, batch_acc = sess.run(\n",
    "                    [error, accuracy], \n",
    "                    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "                valid_error += batch_error\n",
    "                valid_accuracy += batch_acc\n",
    "            valid_error /= valid_data.num_batches\n",
    "            valid_accuracy /= valid_data.num_batches\n",
    "            print('                 err(valid)={0:.2f} acc(valid)={1:.2f}'\n",
    "                   .format(valid_error, valid_accuracy))\n",
    "            acc_valids.append(valid_accuracy)\n",
    "            err_valids.append(valid_error)\n",
    "all_train_score_d[experiment_name] = [acc_train_list,err_train_list]\n",
    "all_valid_score_d[experiment_name] = [acc_valids,err_valids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_plot_graph(err_train_list,acc_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_plot_graph(err_list,acc_list,scale=None):\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(211)\n",
    "    if not scale:\n",
    "        ax_1.plot(acc_list,label='accuration list')\n",
    "    else:\n",
    "        ax_1.plot(np.arange(1,100,scale),acc_list,label='accuration list')\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    \n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(212)\n",
    "    if not scale:\n",
    "        ax_2.plot(err_list,label='error list',c='r')\n",
    "    else:\n",
    "        ax_2.plot(np.arange(1,120,scale),err_list,label='error list',c='r')\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_plot_result(filelist,valid=False,_title=None):\n",
    "    result_dict = {}\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    scale = 5\n",
    "    if _title:\n",
    "        print(_title)\n",
    "        plt.title(_title)\n",
    "    for filetup in filelist:\n",
    "        filename,label=filetup\n",
    "        filepath= \"/afs/inf.ed.ac.uk/user/s15/s1575408/mlpractical/script/\"+filename+\".txt\"\n",
    "        result_dict[filename]=open_file(filepath)\n",
    "        if valid:\n",
    "            plt.plot(np.arange(4,121,scale),result_dict[filename],label=label)\n",
    "        else:\n",
    "            plt.plot(result_dict[filename],label=label)\n",
    "        lgd=plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.savefig(_title+\".svg\",bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pltsin(ax, colors=['b']):\n",
    "    x = np.linspace(0,1,100)\n",
    "    if ax.lines:\n",
    "        for line in ax.lines:\n",
    "            line.set_xdata(x)\n",
    "            y = np.random.random(size=(100,1))\n",
    "            line.set_ydata(y)\n",
    "    else:\n",
    "        for color in colors:\n",
    "            y = np.random.random(size=(100,1))\n",
    "            ax.plot(x, y, color)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filelist =  [(\"acc.relu_50.train\",\"relu-50 neurons\"),(\"acc.relu_500.train\",\"relu-500 neurons\")\n",
    ",(\"acc_trains_model1\",\"softmax-1024 neurons\"),(\"acc_trains_model2\",\"relu-1024 neurons\"),\n",
    "             (\"acc_trains_model3\",\"relu-200 neurons\"),(\"acc_trains_model4\",\"tanh-200 neurons\")]\n",
    "plot_result(filelist, _title='train accuracy experiment 1')\n",
    "filelist = [(\"accrelu_50.valid\",\"relu-50 neurons\"),(\"accrelu_500.valid\",\"relu-500 neurons\")\n",
    ",(\"acc_valid_model1\",\"softmax-1024 neurons\"),(\"acc_valid_model2\",\"relu-1024 neurons\"),\n",
    "            (\"acc_valid_model3\",\"relu-200 neurons\"),(\"acc_valid_model4\",\"tanh-200 neurons\")]\n",
    "plot_result(filelist,valid=True,_title=\"validation accuracy experiment 1\")\n",
    "\n",
    "filelist =  [(\"err.relu_50.train\",\"relu-50 neurons\"),(\"err.relu_500.train\",\"relu-500 neurons\"),(\"error_trains_model1\",\"softmax-1024 neurons\"),(\"error_trains_model2\",\"relu-1024 neurons\"),\n",
    "             (\"error_trains_model3\",\"relu-200 neurons\"),(\"error_trains_model4\",\"tanh-200 neurons\")]\n",
    "plot_result(filelist, _title='train error experiment 1')\n",
    "filelist = [(\"err.relu_50.valid\",\"relu-50 neurons\"),(\"err.relu_500.valid\",\"relu-500 neurons\"),(\"error_valid_model1\",\"softmax-1024 neurons\"),(\"error_valid_model2\",\"relu-1024 neurons\"),\n",
    "            (\"error_valid_model3\",\"relu-200 neurons\"),(\"error_valid_model4\",\"tanh-200 neurons\")]\n",
    "plot_result(filelist,valid=True,_title='validation error experiment 1')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlp]",
   "language": "python",
   "name": "conda-env-mlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
